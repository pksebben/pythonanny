PYTHONANNY DEVLOG

check out python debuggers to look at implementations

So I have a couple of steps to continue:
 - find the implementation of sys.excepthook in the demtutor files
 - see if it sheds light on how to handle things
 - patch it in if it does  //  read up on the docs if it don't

okay so it works sorta.  Now to test danger method and if it works commit
well, pom is up and the danger test is not yet working.  Time to commit to another pomodoro.

Sunday, August 30, 2020

23:28:10: I need to turn in, but I'm in the middle of crawling through the python pdb module to get a sense of how they accomplish wrapping code.  I want to try and grok as much of that as possible and implement it here.

I had a flash of sleepy inspiration:

The output of this is only going to be relevant if it's provided in the right context, by which I mean: the errors you throw have to do with a complex system of all the variables at play when you're programming.  The set of things at work is very large, so instead of attempting to narrow it down at all, perhaps the best strategy is to provide as much of the context within which the error was thrown as possible.

What I would *love* to build is a version control system that integrates with error logging and postmortem tools.  Bonus points if I can somehow create feedback that propagates to a system responsible for optimizing the path to remediation.

I want to be able to sort by these:
- error codes by module
- error codes by date
- error codes by 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Monday, August 31, 2020
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
18:27:51:
Problem: I need to build a generic wrapper around python processes that can be used to process all code on a system

Problem: The way python is structured does not seem to allow for such broad use cases

Solution 1: Build a tool in the Python/C API that does what I want

Solution 2: Continue to analyse and learn pdb and use those methods

Problem: I've never used pdb.  I don't even know if it looks like what I want

Solution: use pdb.  Take a tutorial.  Make it quick.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Monday, September 7, 2020
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

maybe i did stuff this day, maybe not.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Friday, October 16, 2020
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Alright! So I have picked this project as the first thing to polish off IOT build out my CV, as per Dave Fecak's instructions.

Finished looks like:
 - can be easily implemented in a python project
 - code is clean and easy to understand

bonus round!
 - can be integrated into a python coding environment such that you don't have to import it all the time.

Ian pointed me in the direction of https://docs.python.org/3/library/site.html?fbclid=IwAR3L6OgJxvWPsON6f1fAm8j2H-sBrsLC_TVLoB-0nniw1LJjSnBlv_jJi3I

...this will allow me to attach to a venv, and have my code run any time code is run in the venv.  This is basically perfect for what I want to get done.

That said, it looks like pythonanny is fundamentally borked at the moment.  I need to get a working test before I can package the module properly.  I was following the directions here:
https://python-packaging-tutorial.readthedocs.io/en/latest/setup_py.html

..and once I have a basic working copy, I can get back to those directions.

Okay.  Let's follow the logic backwards, starting with sys.excepthook.

Boom!  I think.  I should avoid calling nanny_log directly.

alright, so raising an exception in the tests doesn't seem to work (although, test_error_logged seems to be erroneously passing for some reason...)

Okay.  Nanny is properly handling stuff.  Problem:  It's hard to test that the excepthook is working properly.  Solution:  do this thing
https://stackoverflow.com/questions/46310034/how-to-test-that-a-custom-excepthook-is-installed-correctly

LOG_TAG #0001
++++++++++++++++++++++++++++++++++++++++++++++++++
So I'm now coming up against a borked test case.  Running "failtest.py" most assuredly adds an entry to the log.  For some reason, this does not occur in the test module.

Does the subprocess command work outside of the test environment?

does os.stat record a difference, so long as you do it before and after the tests?

[[[
1 - nanny.log checked.  14 lines.
2 - tests run (tests assert no change to logfile)
3 - nanny.log checked. 14 lines.  No change.
]]]

- previous asserts that when the failure code in the test is run, nothing gets passed to the log.  Potential problems:
1 - there is something about the test module that isolates writes. (perhaps nanny.init() is not actually called in the right order?)
1a[[[
nanny import

]]]

Q- Am I calling subprocess the same in both cases?

Q- Does just running the failtest by itself add a log?
YES! It does!

Q- Does running the failtest in subproc in an interactive shell add a log?
start @ 15 logs
IT TOTALLY DOES!

Okay, so that was weird, but simply moving the Popen into the test (instead of containing it in it's own function) did the trick.  Whert deferq.

keeping the borked code in there, commented out for forensics.
++++++++++++++++++++++++++++++++++++++++++++++++++


Let's take inventory, right quick...
Tests are working.
Logging is working.

TODO:
Get rid of the need to call init()
Ensure that the logs are useful
modularize this so it works with venvs.
implement some actual _usage_ of the collected data.  Otherwise what's the damn point?

So I feel like the actual capture part of this app is (mostly) done (read:functional).

hey, this is a good time to git, innit?
